{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\raghu\\miniconda3\\lib\\site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in c:\\users\\raghu\\miniconda3\\lib\\site-packages (0.0.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\users\\raghu\\miniconda3\\lib\\site-packages (1.22.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\raghu\\miniconda3\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\raghu\\miniconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\raghu\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.8.1 regex-2022.10.31\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip  install keras\n",
    "%pip install sklearn\n",
    "%pip install numpy\n",
    "%pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification using Deep Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # URL removed\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # mention and hashtag removed\n",
    "    text = re.sub(r'@\\S+|#S+', '', text)\n",
    "    # non-ASCII removed\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # numbers removed\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # whitespaces removed\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # punctuations removed\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # 'rt' / 'retweet' removed\n",
    "    text = re.sub(r'\\b(rt|retweet)\\b', '', text.lower())\n",
    "    # stopwords removed\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    #text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    # Lemmatization\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "tweets_df = pd.read_csv('twitterData.csv')\n",
    "tweets_df['cleaned_text'] = tweets_df['text'].apply(preprocess_text)\n",
    "tweets_df.drop_duplicates(inplace=True)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = tweets_df['cleaned_text'].values.tolist()\n",
    "y = le.fit_transform(tweets_df['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
    "    return (X_train,X_test)\n",
    "\n",
    "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
    "    \"\"\"\n",
    "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
    "    Build Deep neural networks Model for text classification\n",
    "    Shape is input feature space\n",
    "    nClasses is number of classes\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    node = 512 # number of nodes\n",
    "    nLayers = 4 # number of  hidden layer\n",
    "\n",
    "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(0,nLayers):\n",
    "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 13941 features\n",
      "Epoch 1/10\n",
      "34/34 - 6s - loss: 2.4121 - accuracy: 0.1418 - val_loss: 2.1604 - val_accuracy: 0.1274 - 6s/epoch - 178ms/step\n",
      "Epoch 2/10\n",
      "34/34 - 5s - loss: 2.1237 - accuracy: 0.1522 - val_loss: 2.0089 - val_accuracy: 0.2764 - 5s/epoch - 158ms/step\n",
      "Epoch 3/10\n",
      "34/34 - 5s - loss: 1.7543 - accuracy: 0.3067 - val_loss: 1.4753 - val_accuracy: 0.5104 - 5s/epoch - 148ms/step\n",
      "Epoch 4/10\n",
      "34/34 - 5s - loss: 0.8682 - accuracy: 0.6831 - val_loss: 0.9867 - val_accuracy: 0.7038 - 5s/epoch - 151ms/step\n",
      "Epoch 5/10\n",
      "34/34 - 5s - loss: 0.4290 - accuracy: 0.8487 - val_loss: 0.9522 - val_accuracy: 0.7679 - 5s/epoch - 153ms/step\n",
      "Epoch 6/10\n",
      "34/34 - 5s - loss: 0.2316 - accuracy: 0.9212 - val_loss: 1.1225 - val_accuracy: 0.7528 - 5s/epoch - 162ms/step\n",
      "Epoch 7/10\n",
      "34/34 - 6s - loss: 0.1502 - accuracy: 0.9467 - val_loss: 0.9955 - val_accuracy: 0.8047 - 6s/epoch - 164ms/step\n",
      "Epoch 8/10\n",
      "34/34 - 4s - loss: 0.1105 - accuracy: 0.9667 - val_loss: 0.9528 - val_accuracy: 0.8189 - 4s/epoch - 128ms/step\n",
      "Epoch 9/10\n",
      "34/34 - 5s - loss: 0.0813 - accuracy: 0.9771 - val_loss: 0.9931 - val_accuracy: 0.8208 - 5s/epoch - 136ms/step\n",
      "Epoch 10/10\n",
      "34/34 - 5s - loss: 0.0710 - accuracy: 0.9821 - val_loss: 1.0507 - val_accuracy: 0.7830 - 5s/epoch - 152ms/step\n",
      "34/34 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82        69\n",
      "           1       0.64      0.81      0.72       171\n",
      "           2       0.84      0.73      0.78       135\n",
      "           3       0.85      0.85      0.85       124\n",
      "           4       0.77      0.84      0.81       122\n",
      "           5       0.82      0.77      0.80        84\n",
      "           6       0.76      0.71      0.74       156\n",
      "           7       0.95      0.70      0.81       151\n",
      "           8       0.83      0.83      0.83        23\n",
      "           9       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.78      1060\n",
      "   macro avg       0.80      0.80      0.80      1060\n",
      "weighted avg       0.80      0.78      0.78      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf,X_test_tfidf = TFIDF(X_train,X_test)\n",
    "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 20)\n",
    "model_DNN.fit(X_train_tfidf, y_train,\n",
    "                              validation_data=(X_test_tfidf, y_test),\n",
    "                              epochs=10,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_DNN.predict(X_test_tfidf)\n",
    "\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Deep Neural Network for text classification\n",
      "accuracy is 0.7830188679245284\n",
      "MCC: 0.7544188545178457\n",
      "Macro-Averaging Metrics:\n",
      "Accuracy: 0.78\n",
      "Precision: 0.80\n",
      "Recall: 0.80\n",
      "F1 Score: 0.80\n",
      "\n",
      "Micro-Averaging Metrics:\n",
      "Accuracy: 0.78\n",
      "Precision: 0.78\n",
      "Recall: 0.78\n",
      "F1 Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Deep Neural Network for text classification\")\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_test,np.argmax(predicted, axis=1)))\n",
    "mcc = matthews_corrcoef(y_test, np.argmax(predicted, axis=1))\n",
    "print(\"MCC:\", mcc)\n",
    "# Macro-Averaging\n",
    "macro_accuracy = accuracy_score(y_test, np.argmax(predicted, axis=1))\n",
    "macro_precision, macro_recall, macro_f1, _ = score(y_test, np.argmax(predicted, axis=1), average='macro')\n",
    "print(\"Macro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {macro_accuracy:.2f}\")\n",
    "print(f\"Precision: {macro_precision:.2f}\")\n",
    "print(f\"Recall: {macro_recall:.2f}\")\n",
    "print(f\"F1 Score: {macro_f1:.2f}\")\n",
    "# Micro-Averaging\n",
    "micro_accuracy = accuracy_score(y_test, np.argmax(predicted, axis=1))\n",
    "micro_precision, micro_recall, micro_f1, _ = score(y_test, np.argmax(predicted, axis=1), average='micro')\n",
    "print(\"\\nMicro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {micro_accuracy:.2f}\")\n",
    "print(f\"Precision: {micro_precision:.2f}\")\n",
    "print(f\"Recall: {micro_recall:.2f}\")\n",
    "print(f\"F1 Score: {micro_f1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###without stopwords & lemmetization!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       216\n",
      "           1       0.78      0.86      0.82       181\n",
      "           2       0.90      0.78      0.84       215\n",
      "           3       0.91      0.83      0.87       188\n",
      "           4       0.90      0.90      0.90       196\n",
      "           5       0.97      0.96      0.96       201\n",
      "           6       0.84      0.87      0.85       217\n",
      "           7       0.92      0.90      0.91       192\n",
      "           8       0.56      0.97      0.71        31\n",
      "           9       0.80      0.95      0.87        21\n",
      "\n",
      "    accuracy                           0.89      1658\n",
      "   macro avg       0.86      0.90      0.87      1658\n",
      "weighted avg       0.90      0.89      0.89      1658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, GRU, Embedding\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\".\\\\glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n",
    "\n",
    "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    \"\"\"\n",
    "    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "    word_index in word index ,\n",
    "    embeddings_index is embeddings index, look at data_helper.py\n",
    "    nClasses is number of classes,\n",
    "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    hidden_layer = 3\n",
    "    gru_node = 32\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
    "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
    "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "\n",
    "\n",
    "    print(gru_node)\n",
    "    for i in range(0,hidden_layer):\n",
    "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(nclasses, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15987 unique tokens.\n",
      "(5298, 500)\n",
      "Total 1193514 word vectors.\n",
      "32\n",
      "Epoch 1/10\n",
      "34/34 - 114s - loss: 2.5342 - accuracy: 0.1402 - val_loss: 2.2034 - val_accuracy: 0.1500 - 114s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "34/34 - 92s - loss: 2.1918 - accuracy: 0.1498 - val_loss: 2.1173 - val_accuracy: 0.2028 - 92s/epoch - 3s/step\n",
      "Epoch 3/10\n",
      "34/34 - 92s - loss: 2.0629 - accuracy: 0.2006 - val_loss: 1.9480 - val_accuracy: 0.2406 - 92s/epoch - 3s/step\n",
      "Epoch 4/10\n",
      "34/34 - 113s - loss: 1.8947 - accuracy: 0.2529 - val_loss: 1.8365 - val_accuracy: 0.2632 - 113s/epoch - 3s/step\n",
      "Epoch 5/10\n",
      "34/34 - 120s - loss: 1.7545 - accuracy: 0.3134 - val_loss: 1.7259 - val_accuracy: 0.3491 - 120s/epoch - 4s/step\n",
      "Epoch 6/10\n",
      "34/34 - 118s - loss: 1.6058 - accuracy: 0.3952 - val_loss: 1.5728 - val_accuracy: 0.4358 - 118s/epoch - 3s/step\n",
      "Epoch 7/10\n",
      "34/34 - 117s - loss: 1.4491 - accuracy: 0.4571 - val_loss: 1.4312 - val_accuracy: 0.5038 - 117s/epoch - 3s/step\n",
      "Epoch 8/10\n",
      "34/34 - 116s - loss: 1.2818 - accuracy: 0.5415 - val_loss: 1.3642 - val_accuracy: 0.5632 - 116s/epoch - 3s/step\n",
      "Epoch 9/10\n",
      "34/34 - 120s - loss: 1.0947 - accuracy: 0.6279 - val_loss: 1.2408 - val_accuracy: 0.6198 - 120s/epoch - 4s/step\n",
      "Epoch 10/10\n",
      "34/34 - 279s - loss: 0.9654 - accuracy: 0.6878 - val_loss: 1.1906 - val_accuracy: 0.6651 - 279s/epoch - 8s/step\n",
      "34/34 [==============================] - 10s 266ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        69\n",
      "           1       0.58      0.73      0.65       171\n",
      "           2       0.78      0.54      0.64       135\n",
      "           3       0.53      0.65      0.58       124\n",
      "           4       0.84      0.76      0.80       122\n",
      "           5       0.65      0.60      0.62        84\n",
      "           6       0.62      0.76      0.68       156\n",
      "           7       0.76      0.72      0.74       151\n",
      "           8       0.50      0.22      0.30        23\n",
      "           9       0.80      0.16      0.27        25\n",
      "\n",
      "    accuracy                           0.67      1060\n",
      "   macro avg       0.68      0.58      0.60      1060\n",
      "weighted avg       0.68      0.67      0.66      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n",
    "\n",
    "model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 20)\n",
    "\n",
    "model_RNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=10,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_RNN.predict(X_test_Glove)\n",
    "\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Recurrent Neural Network for text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72        69\n",
      "           1       0.58      0.73      0.65       171\n",
      "           2       0.78      0.54      0.64       135\n",
      "           3       0.53      0.65      0.58       124\n",
      "           4       0.84      0.76      0.80       122\n",
      "           5       0.65      0.60      0.62        84\n",
      "           6       0.62      0.76      0.68       156\n",
      "           7       0.76      0.72      0.74       151\n",
      "           8       0.50      0.22      0.30        23\n",
      "           9       0.80      0.16      0.27        25\n",
      "\n",
      "    accuracy                           0.67      1060\n",
      "   macro avg       0.68      0.58      0.60      1060\n",
      "weighted avg       0.68      0.67      0.66      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Recurrent Neural Network for text classification\")\n",
    "print(metrics.classification_report(y_test, np.argmax(predicted, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Recurrent Neural Network for text classification\n",
      "accuracy is 0.6650943396226415\n",
      "MCC: 0.6178581905583603\n",
      "Macro-Averaging Metrics:\n",
      "Accuracy: 0.67\n",
      "Precision: 0.68\n",
      "Recall: 0.58\n",
      "F1 Score: 0.60\n",
      "\n",
      "Micro-Averaging Metrics:\n",
      "Accuracy: 0.67\n",
      "Precision: 0.67\n",
      "Recall: 0.67\n",
      "F1 Score: 0.67\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Recurrent Neural Network for text classification\")\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_test,np.argmax(predicted, axis=1)))\n",
    "mcc = matthews_corrcoef(y_test, np.argmax(predicted, axis=1))\n",
    "print(\"MCC:\", mcc)\n",
    "# Macro-Averaging\n",
    "macro_accuracy = accuracy_score(y_test, np.argmax(predicted, axis=1))\n",
    "macro_precision, macro_recall, macro_f1, _ = score(y_test, np.argmax(predicted, axis=1), average='macro')\n",
    "print(\"Macro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {macro_accuracy:.2f}\")\n",
    "print(f\"Precision: {macro_precision:.2f}\")\n",
    "print(f\"Recall: {macro_recall:.2f}\")\n",
    "print(f\"F1 Score: {macro_f1:.2f}\")\n",
    "# Micro-Averaging\n",
    "micro_accuracy = accuracy_score(y_test, np.argmax(predicted, axis=1))\n",
    "micro_precision, micro_recall, micro_f1, _ = score(y_test, np.argmax(predicted, axis=1), average='micro')\n",
    "print(\"\\nMicro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {micro_accuracy:.2f}\")\n",
    "print(f\"Precision: {micro_precision:.2f}\")\n",
    "print(f\"Recall: {micro_recall:.2f}\")\n",
    "print(f\"F1 Score: {micro_f1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense,Input,Embedding,Flatten, MaxPooling1D, Conv1D\n",
    "from keras.models import Sequential,Model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\".\\\\glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n",
    "\n",
    "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "\n",
    "    \"\"\"\n",
    "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
    "        word_index in word index ,\n",
    "        embeddings_index is embeddings index, look at data_helper.py\n",
    "        nClasses is number of classes,\n",
    "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
    "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "\n",
    "    # applying a more complex convolutional approach\n",
    "    convs = []\n",
    "    filter_sizes = []\n",
    "    layer = 5\n",
    "    print(\"Filter  \",layer)\n",
    "    for fl in range(0,layer):\n",
    "        filter_sizes.append((fl+2))\n",
    "\n",
    "    node = 128\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(5)(l_conv)\n",
    "        #l_pool = Dropout(0.25)(l_pool)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
    "    l_cov1 = Dropout(dropout)(l_cov1)\n",
    "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
    "    l_cov2 = Dropout(dropout)(l_cov2)\n",
    "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "    l_flat = Flatten()(l_pool2)\n",
    "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    l_dense = Dense(512, activation='relu')(l_dense)\n",
    "    l_dense = Dropout(dropout)(l_dense)\n",
    "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
    "    model = Model(sequence_input, preds)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15987 unique tokens.\n",
      "(5298, 500)\n",
      "Total 1193514 word vectors.\n",
      "Filter   5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 500, 50)      799400      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 499, 128)     12928       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 498, 128)     19328       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 497, 128)     25728       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 496, 128)     32128       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 495, 128)     38528       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 99, 128)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 495, 128)     0           ['max_pooling1d[0][0]',          \n",
      "                                                                  'max_pooling1d_1[0][0]',        \n",
      "                                                                  'max_pooling1d_2[0][0]',        \n",
      "                                                                  'max_pooling1d_3[0][0]',        \n",
      "                                                                  'max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 491, 128)     82048       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 491, 128)     0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 98, 128)     0           ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 94, 128)      82048       ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 94, 128)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 3, 128)      0           ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 384)          0           ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1024)         394240      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 1024)         0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 512)          524800      ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 512)          0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 20)           10260       ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,021,436\n",
      "Trainable params: 2,021,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "34/34 - 194s - loss: 2.3816 - accuracy: 0.1425 - val_loss: 2.4531 - val_accuracy: 0.1830 - 194s/epoch - 6s/step\n",
      "Epoch 2/15\n",
      "34/34 - 184s - loss: 2.2293 - accuracy: 0.1477 - val_loss: 2.5105 - val_accuracy: 0.1642 - 184s/epoch - 5s/step\n",
      "Epoch 3/15\n",
      "34/34 - 158s - loss: 2.1797 - accuracy: 0.1579 - val_loss: 2.3385 - val_accuracy: 0.1538 - 158s/epoch - 5s/step\n",
      "Epoch 4/15\n",
      "34/34 - 166s - loss: 2.0040 - accuracy: 0.2272 - val_loss: 2.1465 - val_accuracy: 0.2802 - 166s/epoch - 5s/step\n",
      "Epoch 5/15\n",
      "34/34 - 164s - loss: 1.8178 - accuracy: 0.2855 - val_loss: 1.9283 - val_accuracy: 0.3708 - 164s/epoch - 5s/step\n",
      "Epoch 6/15\n",
      "34/34 - 162s - loss: 1.5464 - accuracy: 0.4054 - val_loss: 1.6802 - val_accuracy: 0.4698 - 162s/epoch - 5s/step\n",
      "Epoch 7/15\n",
      "34/34 - 169s - loss: 1.2583 - accuracy: 0.5481 - val_loss: 1.4154 - val_accuracy: 0.5868 - 169s/epoch - 5s/step\n",
      "Epoch 8/15\n",
      "34/34 - 174s - loss: 1.0266 - accuracy: 0.6428 - val_loss: 1.2464 - val_accuracy: 0.6500 - 174s/epoch - 5s/step\n",
      "Epoch 9/15\n",
      "34/34 - 174s - loss: 0.7927 - accuracy: 0.7350 - val_loss: 1.1235 - val_accuracy: 0.6896 - 174s/epoch - 5s/step\n",
      "Epoch 10/15\n",
      "34/34 - 173s - loss: 0.6613 - accuracy: 0.7857 - val_loss: 1.0164 - val_accuracy: 0.7208 - 173s/epoch - 5s/step\n",
      "Epoch 11/15\n",
      "34/34 - 158s - loss: 0.5120 - accuracy: 0.8336 - val_loss: 0.9194 - val_accuracy: 0.7283 - 158s/epoch - 5s/step\n",
      "Epoch 12/15\n",
      "34/34 - 161s - loss: 0.3928 - accuracy: 0.8787 - val_loss: 0.8408 - val_accuracy: 0.7547 - 161s/epoch - 5s/step\n",
      "Epoch 13/15\n",
      "34/34 - 159s - loss: 0.2984 - accuracy: 0.9096 - val_loss: 0.7915 - val_accuracy: 0.7642 - 159s/epoch - 5s/step\n",
      "Epoch 14/15\n",
      "34/34 - 153s - loss: 0.2331 - accuracy: 0.9278 - val_loss: 0.8088 - val_accuracy: 0.7660 - 153s/epoch - 5s/step\n",
      "Epoch 15/15\n",
      "34/34 - 171s - loss: 0.1968 - accuracy: 0.9346 - val_loss: 0.8272 - val_accuracy: 0.7670 - 171s/epoch - 5s/step\n",
      "34/34 [==============================] - 5s 143ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n",
    "model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
    "model_CNN.summary()\n",
    "model_CNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_CNN.predict(X_test_Glove)\n",
    "predicted = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Convolutional Neural Network for text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87        69\n",
      "           1       0.71      0.80      0.75       171\n",
      "           2       0.68      0.72      0.70       135\n",
      "           3       0.84      0.75      0.79       124\n",
      "           4       0.81      0.79      0.80       122\n",
      "           5       0.79      0.79      0.79        84\n",
      "           6       0.69      0.74      0.72       156\n",
      "           7       0.88      0.77      0.82       151\n",
      "           8       0.61      0.83      0.70        23\n",
      "           9       0.81      0.68      0.74        25\n",
      "\n",
      "    accuracy                           0.77      1060\n",
      "   macro avg       0.78      0.77      0.77      1060\n",
      "weighted avg       0.78      0.77      0.77      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Convolutional Neural Network for text classification\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Convolutional Neural Network for text classification\n",
      "accuracy is 0.7669811320754717\n",
      "MCC: 0.7346007746743662\n",
      "Macro-Averaging Metrics:\n",
      "Accuracy: 0.77\n",
      "Precision: 0.78\n",
      "Recall: 0.77\n",
      "F1 Score: 0.77\n",
      "\n",
      "Micro-Averaging Metrics:\n",
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "F1 Score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Convolutional Neural Network for text classification\")\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_test,predicted))\n",
    "mcc = matthews_corrcoef(y_test, predicted)\n",
    "print(\"MCC:\", mcc)\n",
    "# Macro-Averaging\n",
    "macro_accuracy = accuracy_score(y_test, predicted)\n",
    "macro_precision, macro_recall, macro_f1, _ = score(y_test, predicted, average='macro')\n",
    "print(\"Macro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {macro_accuracy:.2f}\")\n",
    "print(f\"Precision: {macro_precision:.2f}\")\n",
    "print(f\"Recall: {macro_recall:.2f}\")\n",
    "print(f\"F1 Score: {macro_f1:.2f}\")\n",
    "# Micro-Averaging\n",
    "micro_accuracy = accuracy_score(y_test, predicted)\n",
    "micro_precision, micro_recall, micro_f1, _ = score(y_test, predicted, average='micro')\n",
    "print(\"\\nMicro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {micro_accuracy:.2f}\")\n",
    "print(f\"Precision: {micro_precision:.2f}\")\n",
    "print(f\"Recall: {micro_recall:.2f}\")\n",
    "print(f\"F1 Score: {micro_f1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
    "    np.random.seed(7)\n",
    "    text = np.concatenate((X_train, X_test), axis=0)\n",
    "    text = np.array(text)\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    word_index = tokenizer.word_index\n",
    "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    indices = np.arange(text.shape[0])\n",
    "    # np.random.shuffle(indices)\n",
    "    text = text[indices]\n",
    "    print(text.shape)\n",
    "    X_train = text[0:len(X_train), ]\n",
    "    X_test = text[len(X_train):, ]\n",
    "    embeddings_index = {}\n",
    "    f = open(\".\\\\glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "        except:\n",
    "            pass\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Total %s word vectors.' % len(embeddings_index))\n",
    "    return (X_train, X_test, word_index,embeddings_index)\n",
    "\n",
    "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
    "\n",
    "    kernel_size = 2\n",
    "    filters = 256\n",
    "    pool_size = 2\n",
    "    gru_node = 256\n",
    "\n",
    "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
    "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
    "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
    "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
    "                exit(1)\n",
    "\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1024,activation='relu'))\n",
    "    model.add(Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15987 unique tokens.\n",
      "(5298, 500)\n",
      "Total 1193514 word vectors.\n",
      "Filter   5\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 500, 50)      799400      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 499, 128)     12928       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 498, 128)     19328       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 497, 128)     25728       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 496, 128)     32128       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 495, 128)     38528       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 99, 128)     0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 99, 128)     0           ['conv1d_10[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooling1D  (None, 99, 128)     0           ['conv1d_11[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 495, 128)     0           ['max_pooling1d_7[0][0]',        \n",
      "                                                                  'max_pooling1d_8[0][0]',        \n",
      "                                                                  'max_pooling1d_9[0][0]',        \n",
      "                                                                  'max_pooling1d_10[0][0]',       \n",
      "                                                                  'max_pooling1d_11[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 491, 128)     82048       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 491, 128)     0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_12 (MaxPooling1D  (None, 98, 128)     0           ['dropout_18[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 94, 128)      82048       ['max_pooling1d_12[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 94, 128)      0           ['conv1d_13[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling1d_13 (MaxPooling1D  (None, 3, 128)      0           ['dropout_19[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 384)          0           ['max_pooling1d_13[0][0]']       \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1024)         394240      ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 1024)         0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 512)          524800      ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 512)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 20)           10260       ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,021,436\n",
      "Trainable params: 2,021,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "34/34 - 147s - loss: 2.4079 - accuracy: 0.1366 - val_loss: 2.4672 - val_accuracy: 0.1472 - 147s/epoch - 4s/step\n",
      "Epoch 2/15\n",
      "34/34 - 143s - loss: 2.2277 - accuracy: 0.1472 - val_loss: 2.4126 - val_accuracy: 0.1821 - 143s/epoch - 4s/step\n",
      "Epoch 3/15\n",
      "34/34 - 154s - loss: 2.1019 - accuracy: 0.1963 - val_loss: 2.3470 - val_accuracy: 0.2217 - 154s/epoch - 5s/step\n",
      "Epoch 4/15\n",
      "34/34 - 137s - loss: 1.9053 - accuracy: 0.2647 - val_loss: 1.9657 - val_accuracy: 0.3509 - 137s/epoch - 4s/step\n",
      "Epoch 5/15\n",
      "34/34 - 137s - loss: 1.6645 - accuracy: 0.3790 - val_loss: 1.7690 - val_accuracy: 0.4311 - 137s/epoch - 4s/step\n",
      "Epoch 6/15\n",
      "34/34 - 139s - loss: 1.4137 - accuracy: 0.4622 - val_loss: 1.4892 - val_accuracy: 0.5283 - 139s/epoch - 4s/step\n",
      "Epoch 7/15\n",
      "34/34 - 139s - loss: 1.1755 - accuracy: 0.5462 - val_loss: 1.3459 - val_accuracy: 0.6264 - 139s/epoch - 4s/step\n",
      "Epoch 8/15\n",
      "34/34 - 129s - loss: 0.9298 - accuracy: 0.6706 - val_loss: 1.1379 - val_accuracy: 0.6660 - 129s/epoch - 4s/step\n",
      "Epoch 9/15\n",
      "34/34 - 131s - loss: 0.6820 - accuracy: 0.7777 - val_loss: 0.9828 - val_accuracy: 0.7142 - 131s/epoch - 4s/step\n",
      "Epoch 10/15\n",
      "34/34 - 123s - loss: 0.5357 - accuracy: 0.8254 - val_loss: 0.8729 - val_accuracy: 0.7491 - 123s/epoch - 4s/step\n",
      "Epoch 11/15\n",
      "34/34 - 106s - loss: 0.4453 - accuracy: 0.8539 - val_loss: 0.8507 - val_accuracy: 0.7387 - 106s/epoch - 3s/step\n",
      "Epoch 12/15\n",
      "34/34 - 104s - loss: 0.3233 - accuracy: 0.8938 - val_loss: 0.8455 - val_accuracy: 0.7566 - 104s/epoch - 3s/step\n",
      "Epoch 13/15\n",
      "34/34 - 103s - loss: 0.2743 - accuracy: 0.9113 - val_loss: 0.8479 - val_accuracy: 0.7415 - 103s/epoch - 3s/step\n",
      "Epoch 14/15\n",
      "34/34 - 112s - loss: 0.2273 - accuracy: 0.9259 - val_loss: 0.8326 - val_accuracy: 0.7406 - 112s/epoch - 3s/step\n",
      "Epoch 15/15\n",
      "34/34 - 119s - loss: 0.1735 - accuracy: 0.9460 - val_loss: 0.8613 - val_accuracy: 0.7415 - 119s/epoch - 4s/step\n",
      "34/34 [==============================] - 4s 114ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)\n",
    "\n",
    "model_RCNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
    "model_RCNN.summary()\n",
    "model_RCNN.fit(X_train_Glove, y_train,\n",
    "                              validation_data=(X_test_Glove, y_test),\n",
    "                              epochs=15,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model_RCNN.predict(X_test_Glove)\n",
    "predicted = np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Recurrent Convolutional Neural Network for text classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.78      0.84        69\n",
      "           1       0.62      0.84      0.71       171\n",
      "           2       0.75      0.64      0.69       135\n",
      "           3       0.84      0.69      0.76       124\n",
      "           4       0.90      0.75      0.82       122\n",
      "           5       0.59      0.77      0.67        84\n",
      "           6       0.69      0.74      0.72       156\n",
      "           7       0.88      0.77      0.82       151\n",
      "           8       0.64      0.61      0.62        23\n",
      "           9       0.82      0.56      0.67        25\n",
      "\n",
      "    accuracy                           0.74      1060\n",
      "   macro avg       0.76      0.72      0.73      1060\n",
      "weighted avg       0.76      0.74      0.74      1060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Recurrent Convolutional Neural Network for text classification\")\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Recurrent Convolutional Neural Network for text classification\n",
      "accuracy is 0.7415094339622641\n",
      "MCC: 0.7067780882248423\n",
      "Macro-Averaging Metrics:\n",
      "Accuracy: 0.74\n",
      "Precision: 0.76\n",
      "Recall: 0.72\n",
      "F1 Score: 0.73\n",
      "\n",
      "Micro-Averaging Metrics:\n",
      "Accuracy: 0.74\n",
      "Precision: 0.74\n",
      "Recall: 0.74\n",
      "F1 Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance of Recurrent Convolutional Neural Network for text classification\")\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_test,predicted))\n",
    "mcc = matthews_corrcoef(y_test, predicted)\n",
    "print(\"MCC:\", mcc)\n",
    "# Macro-Averaging\n",
    "macro_accuracy = accuracy_score(y_test, predicted)\n",
    "macro_precision, macro_recall, macro_f1, _ = score(y_test, predicted, average='macro')\n",
    "print(\"Macro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {macro_accuracy:.2f}\")\n",
    "print(f\"Precision: {macro_precision:.2f}\")\n",
    "print(f\"Recall: {macro_recall:.2f}\")\n",
    "print(f\"F1 Score: {macro_f1:.2f}\")\n",
    "# Micro-Averaging\n",
    "micro_accuracy = accuracy_score(y_test, predicted)\n",
    "micro_precision, micro_recall, micro_f1, _ = score(y_test, predicted, average='micro')\n",
    "print(\"\\nMicro-Averaging Metrics:\")\n",
    "print(f\"Accuracy: {micro_accuracy:.2f}\")\n",
    "print(f\"Precision: {micro_precision:.2f}\")\n",
    "print(f\"Recall: {micro_recall:.2f}\")\n",
    "print(f\"F1 Score: {micro_f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
